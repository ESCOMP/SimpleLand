{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a SLIM forcing file from CTSM output\n",
    "by S. Levis, \n",
    "modified from pres_vs_hist_alb_LN_postAGU20190621_corrected_rs.ipynb by Marysa Lague"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import time as tm\n",
    "from copy import copy \n",
    "\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accurate masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER MODIFY section\n",
    "# -------------------\n",
    "# CTSM history file from archived simulation that used the fsurdat file a few lines down\n",
    "# simulation must be a bgc case, so as to include the history variable HTOP\n",
    "casename = 'ihist_bgccrop'  # USER MODIFY\n",
    "start_yr = '2014'  # USER MODIFY\n",
    "\n",
    "username = os.environ.get('USER')\n",
    "case_archive_dir = '/glade/scratch/' + username + '/archive/' + casename  # USER may need to modify\n",
    "ctsm_dir = case_archive_dir + '/lnd/hist/'  # USER may need to modify\n",
    "ctsm_file = ctsm_dir + casename + '.clm2.h0.' + start_yr + '-01.nc'  # USER may need to modify\n",
    "\n",
    "if os.path.exists(ctsm_file):\n",
    "    ctsm_ds = xr.open_dataset(ctsm_file)\n",
    "else:\n",
    "    errmsg = \"ctsm_file does not exist: \" + ctsm_file\n",
    "    sys.exit(errmsg)\n",
    "\n",
    "lat_ctsm = ctsm_ds.variables['lat'].values[:]\n",
    "lon_ctsm = ctsm_ds.variables['lon'].values[:]\n",
    "landmask = ctsm_ds.landmask.values[:]\n",
    "dims = np.shape(landmask)\n",
    "\n",
    "# USER MODIFY section\n",
    "# -------------------\n",
    "# ctsm fsurdat file from simulation that produced the ctsm history file a few lines up\n",
    "# if ctsm fsurdat file does not exist, glc_mask will equal zero everywhere\n",
    "surfdat_dir ='/glade/p/cesmdata/cseg/inputdata/lnd/clm2/surfdata_map/release-clm5.0.18/'  # USER may need to modify\n",
    "surfdat_file = surfdat_dir + 'surfdata_0.9x1.25_hist_78pfts_CMIP6_simyr2000_c190214.nc'  # USER may wish to modify\n",
    "\n",
    "glc_mask = np.zeros(dims)\n",
    "if os.path.exists(surfdat_file):\n",
    "    surfdat_ds = xr.open_dataset(surfdat_file)\n",
    "    # get glacier mask\n",
    "    glc_pct = (surfdat_ds.variables['PCT_GLACIER']).values[:]\n",
    "    # apply the glacier mask where glc_pct > 50% \n",
    "    glc_mask[glc_pct > 50] = 1\n",
    "else:\n",
    "    surfdata_file = ''\n",
    "\n",
    "dirt_mask = np.where((landmask==1.) & (glc_mask==0.), 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# USER-GENERATED FILES\n",
    "# Load nco-generated ctsm history file containing monthly averages (ie, avg Jan, avg Feb, etc)\n",
    "# Sample use of nco to concatenate 12 months of a single year:\n",
    "# ncecat <casename>.clm2.h0.<year>-* <casename>.clm2.h0.<year>.nc\n",
    "# nco user's guide: https://nco.sourceforge.net/\n",
    "ctsm_concatenated_file = ctsm_dir + casename + '.clm2.h0.' + start_yr + '.nc'  # USER may need to modify\n",
    "\n",
    "if os.path.exists(ctsm_concatenated_file):\n",
    "    ds = xr.open_dataset(ctsm_concatenated_file, decode_times=False)\n",
    "else:\n",
    "    errmsg = \"ctsm_concatenated_file does not exist: \" + ctsm_concatenated_file\n",
    "    sys.exit(errmsg)\n",
    "\n",
    "ds['time'] = ds['record']  # ncecat (a few lines up) introduced the \"record\" dimension\n",
    "months_per_yr = len(ds['time'].values)\n",
    "print(months_per_yr)  # expect 12\n",
    "\n",
    "# DUST: nco-generated monthly means of coupler history files\n",
    "# if dust_file does not exist, dust fluxes will be set to zero\n",
    "cpl_dir = case_archive_dir + '/cpl/hist/'  # USER may need to modify\n",
    "dust_file = cpl_dir + casename + '.cpl.h0.' + start_yr + '.nc'  # USER may need to modify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctsm history variables used in this script; for informational purposes only\n",
    "# lndvars = ['FSR','FSDS','GSSHA','GSSUN','GSSHALN','GSSUNLN','TLAI','HTOP',\n",
    "#            'LAISUN','LAISHA','SNOW_DEPTH',\n",
    "#            'FSRND','FSRNI','FSRVD','FSRVI',\n",
    "#            'FSDSND','FSDSNI','FSDSVD','FSDSVI' ]\n",
    "# add  mask information to the dataset\n",
    "ds['glc_mask'] = xr.DataArray(dims=['lat','lon'], data=glc_mask)\n",
    "ds['bareground_mask'] = xr.DataArray(dims=['lat','lon'], data=dirt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct albedos from sw reflected / down:\n",
    "#-------ALBEDO------#\n",
    "nameswap = {}\n",
    "nameswap['FSR'] = 'ALBEDO'\n",
    "# albedo:\n",
    "ds_temp = xr.merge([ds['FSR'], ds['FSDS']])\n",
    "ds_temp['ALBEDO'] = ds_temp['FSR'] / ds_temp['FSDS']\n",
    "ds_temp['ALBEDO'].attrs['units'] = 'unitless'\n",
    "ds_temp['ALBEDO'].attrs['longname'] = 'multistream albedo'\n",
    "ds['ALBEDO'] = ds_temp['ALBEDO']\n",
    "\n",
    "ds_temp = xr.merge([ds['FSRND'], ds['FSDSND']])\n",
    "ds_temp['ALBEDO_ND'] = ds_temp['FSRND'] / ds_temp['FSDSND']\n",
    "ds_temp['ALBEDO_ND'].attrs['units'] = 'unitless'\n",
    "ds_temp['ALBEDO_ND'].attrs['longname'] = 'near-IR direct albedo'\n",
    "ds['ALBEDO_ND'] = ds_temp['ALBEDO_ND']\n",
    "\n",
    "ds_temp = xr.merge([ds['FSRNI'], ds['FSDSNI']])\n",
    "ds_temp['ALBEDO_NI'] = ds_temp['FSRNI'] / ds_temp['FSDSNI']\n",
    "ds_temp['ALBEDO_NI'].attrs['units'] = 'unitless'\n",
    "ds_temp['ALBEDO_NI'].attrs['longname'] = 'near-IR diffuse albedo'\n",
    "ds['ALBEDO_NI'] = ds_temp['ALBEDO_NI']\n",
    "    \n",
    "ds_temp = xr.merge([ds['FSRVD'], ds['FSDSVD']])\n",
    "ds_temp['ALBEDO_VD'] = ds_temp['FSRVD'] / ds_temp['FSDSVD']\n",
    "ds_temp['ALBEDO_VD'].attrs['units'] = 'unitless'\n",
    "ds_temp['ALBEDO_VD'].attrs['longname'] = 'visible direct albedo'\n",
    "ds['ALBEDO_VD'] = ds_temp['ALBEDO_VD']\n",
    "\n",
    "ds_temp = xr.merge([ds['FSRVI'], ds['FSDSVI']])\n",
    "ds_temp['ALBEDO_VI'] = ds_temp['FSRVI'] / ds_temp['FSDSVI']\n",
    "ds_temp['ALBEDO_VI'].attrs['units'] = 'unitless'\n",
    "ds_temp['ALBEDO_VI'].attrs['longname'] = 'visible diffuse albedo'\n",
    "ds['ALBEDO_VI'] = ds_temp['ALBEDO_VI']\n",
    "    \n",
    "#-------EVAP RS------#\n",
    "gs_to_rs = 42.3 * 10**6  # umol H20/m2/s   to   s/m\n",
    "ds_temp = xr.merge([ds['GSSUNLN'], ds['GSSHALN'], ds['LAISUN'], ds['LAISHA']])\n",
    "sunLN = ds['GSSUNLN'] * ds['LAISUN']\n",
    "shaLN = ds['GSSHALN'] * ds['LAISHA']\n",
    "ds_temp['evap_rs_LN'] = gs_to_rs / (sunLN + shaLN)\n",
    "ds_temp['evap_rs_LN'].attrs['units'] = 's/m'\n",
    "ds_temp['evap_rs_LN'].attrs['longname'] = 'evaporative resistance at local noon = (42.3 x 10^6)/(gssunln*laisun + gsshaln*laisha)'\n",
    "ds['evap_rs_LN'] = ds_temp['evap_rs_LN']\n",
    "    \n",
    "rs_LN_uncapped = gs_to_rs / (sunLN + shaLN)\n",
    "rs_LN_capped = rs_LN_uncapped.copy()\n",
    "rs_LN_capped.values = np.where(rs_LN_uncapped > 1000., 1000., rs_LN_uncapped)\n",
    "ds_temp['evap_rs_LN_capped'] = rs_LN_capped\n",
    "ds_temp['evap_rs_LN_capped'].attrs['units'] = 's/m'\n",
    "ds_temp['evap_rs_LN_capped'].attrs['longname'] = 'evaporative resistance at local noon capped at 1000 s/m; else = (42.3 x 10^6)/(gssunln*laisun + gsshaln*laisha)'\n",
    "ds['evap_rs_LN_capped'] = ds_temp['evap_rs_LN_capped']\n",
    "    \n",
    "del ds_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snow albedos calculated by Marysa L elsewhere\n",
    "s_alb = {}\n",
    "s_alb['vd'] = 0.97333038\n",
    "s_alb['vi'] = 0.965662\n",
    "s_alb['nd'] = 0.66046935\n",
    "s_alb['ni'] = 0.7067166"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get JJA snowmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_thresh = 0.01  # m\n",
    "snow = ds['SNOW_DEPTH'][6:9,:,:]  # JJA\n",
    "jja_snowfree = np.where(snow > snow_thresh , np.nan , 1.0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the values for albedo, rs, and hc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "alb = {}\n",
    "\n",
    "alb['vd'] = ds['ALBEDO_VD']\n",
    "alb['vi'] = ds['ALBEDO_VI']\n",
    "alb['nd'] = ds['ALBEDO_ND']\n",
    "alb['ni'] = ds['ALBEDO_NI']\n",
    "\n",
    "# make a new albedo field using values anywhere there wasn't snow\n",
    "nc_alb = {}\n",
    "nc_alb['ground'] = {}\n",
    "nc_alb['snow'] = {}\n",
    "\n",
    "# do this in np arrays, not datasets\n",
    "alb_ocn = 0.1  # set ocean points to this generic value\n",
    "for a in alb.keys():\n",
    "    nc_alb['ground'][a] = np.where(jja_snowfree==1., alb[a], alb[a])\n",
    "    \n",
    "    # put snow albedos where glacier mask is true\n",
    "    nc_alb['ground'][a] = np.where(glc_mask==1., s_alb[a], nc_alb['ground'][a])\n",
    "    \n",
    "    # get rid of nans on ocean points\n",
    "    nc_alb['ground'][a] = np.where(np.isnan(nc_alb['ground'][a]), alb_ocn, nc_alb['ground'][a]).mean(axis=0)\n",
    "    \n",
    "    # snow albedo just a single block of color:\n",
    "    nc_alb['snow'][a] = np.ones(np.shape(landmask)) * s_alb[a]\n",
    "    nc_alb['snow'][a] = np.where(np.isnan(nc_alb['snow'][a]), alb_ocn, nc_alb['snow'][a])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rs values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_rs = {}\n",
    "nc_rs = ds['evap_rs_LN_capped']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hc values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From BATS: glacier roughness 0.01  - constant\n",
    "glc_hc = 0.01\n",
    "\n",
    "# doesn't varry seasonally... I don't think... but do the seasonal mod anyhow. For consistency. \n",
    "nc_hc = {}\n",
    "nc_hc = ds['HTOP']\n",
    "\n",
    "# no nans: set to 0.01 (very smooth). \n",
    "hmin=0.01\n",
    "nc_hc = np.where(np.isnan(nc_hc), hmin , nc_hc)\n",
    "\n",
    "# Also no zeros, messes up the turbulence calculation. Make those smooth, too.\n",
    "nc_hc = np.where(nc_hc < hmin, hmin, nc_hc)\n",
    "\n",
    "# set glacier \"height\" to 0.01\n",
    "nc_hc = np.where(glc_mask==1., glc_hc, nc_hc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Other required values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(dust_file):\n",
    "    dust_ds = xr.open_dataset(dust_file)\n",
    "    dust1 = (dust_ds.variables['l2xavg_Fall_flxdst1']).values\n",
    "    dust2 = (dust_ds.variables['l2xavg_Fall_flxdst2']).values\n",
    "    dust3 = (dust_ds.variables['l2xavg_Fall_flxdst3']).values\n",
    "    dust4 = (dust_ds.variables['l2xavg_Fall_flxdst4']).values\n",
    "    # clobber nans\n",
    "    dust1 = np.where(np.isnan(dust1), 0.0, dust1)\n",
    "    dust2 = np.where(np.isnan(dust2), 0.0, dust2)\n",
    "    dust3 = np.where(np.isnan(dust3), 0.0, dust3)\n",
    "    dust4 = np.where(np.isnan(dust4), 0.0, dust4)\n",
    "else:\n",
    "    dust1 = 0\n",
    "    dust2 = 0\n",
    "    dust3 = 0\n",
    "    dust4 = 0\n",
    "    dust_file = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_cv_str = '2e6'\n",
    "soil_cv_val = 2.0e6  # [J/m3/K]\n",
    "soil_tk_val = 1.5  # W/m/K\n",
    "glc_cv = 1.9e6  #  [J/m3/K]\n",
    "glc_tk = 2.4  #  W/m/K\n",
    "snow_mask_depth = 50.0 # kg/m2\n",
    "bucket_capacity = 200.0 # kg/m2\n",
    "\n",
    "# dummy array of ones to extend a lat x lon array into time: mon x lat x lon\n",
    "stretch = np.ones([months_per_yr, dims[0], dims[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a dictionary called nc_data that stores all the variables to be output\n",
    "nc_data = {}    # empty dictionary\n",
    "\n",
    "# glacier mask:\n",
    "nc_data['glc_mask'] = np.copy(glc_mask[:] * stretch)\n",
    "\n",
    "# Albedos as alb_[g-ground/s-snow][v-visible/n-nir][d-direct/f-diffuse]\n",
    "nc_data['alb_gvd'] = nc_alb['ground']['vd'] * stretch\n",
    "nc_data['alb_svd'] = nc_alb['snow']['vd'] * stretch\n",
    "nc_data['alb_gnd'] = nc_alb['ground']['nd'] * stretch\n",
    "nc_data['alb_snd'] = nc_alb['snow']['nd'] * stretch\n",
    "nc_data['alb_gvf'] = nc_alb['ground']['vi'] * stretch\n",
    "nc_data['alb_svf'] = nc_alb['snow']['vi'] * stretch\n",
    "nc_data['alb_gnf'] = nc_alb['ground']['ni'] * stretch\n",
    "nc_data['alb_snf'] = nc_alb['snow']['ni'] * stretch\n",
    "\n",
    "# Bucket capacity in [kg/m2] (or equivalently [mm])\n",
    "nc_data['bucketdepth'] = np.copy(bucket_capacity * stretch)\n",
    "\n",
    "# snow masking \"depth\" in water mass equivalent ([kg/m2] or [mm])\n",
    "nc_data['snowmask'] = np.copy(snow_mask_depth * stretch)\n",
    "\n",
    "# Emissivity (1 = perfect blackbody, not physically realistic)\n",
    "nc_data['emissivity'] = np.copy(1.0 * stretch)\n",
    "\n",
    "# Roughness as \"vegetation height\" [m] (which is then scaled down in the model \n",
    "#       as .1*veg height for actual roughness used)\n",
    "nc_data['roughness'] = nc_hc.mean(axis=0) * stretch\n",
    "\n",
    "# evaporative resistance [s/m] as a sort of \"bulk stomatal resistance\" - actual\n",
    "#       resistance is calculated as a combination of this and how full the bucket is\n",
    "# Initial pass, set all roughness to 100 (this is our \"base\" for glaciers also)\n",
    "nc_data['evap_res'] = nc_rs.mean(axis=0) * stretch\n",
    "\n",
    "# Dust fluxes (from clm4.5 coupled run). There are 4 different dust bins, each\n",
    "#       is given its own field here, to avoid problems I ran into trying to read\n",
    "#       netcdf fields with depth dimensions in the actual model code\n",
    "nc_data['l2xavg_Fall_flxdst1'] = np.copy(dust1 * stretch)\n",
    "nc_data['l2xavg_Fall_flxdst2'] = np.copy(dust2 * stretch)\n",
    "nc_data['l2xavg_Fall_flxdst3'] = np.copy(dust3 * stretch)\n",
    "nc_data['l2xavg_Fall_flxdst4'] = np.copy(dust4 * stretch)\n",
    "\n",
    "# Soil Type (not currently used, set to 0)\n",
    "nc_data['soil_type'] = np.copy(0.0 * stretch)\n",
    "\n",
    "# Thermal Properties\n",
    "#   soil heat capacity cv [J/m3/K] (uniform across column using this definition)\n",
    "# ranges: 1.5e6 for gravel to 3 for clay/silt; 4.2 for water (if we go very saturated, but the dirt'll still be in there...)\n",
    "nc_data['soil_cv_1d'] = np.copy(soil_cv_val * stretch)\n",
    "\n",
    "#   soil thermal conductivity tk [W/m/K] (uniform across column using this definition)\n",
    "nc_data['soil_tk_1d'] = np.copy(soil_tk_val * stretch)\n",
    "\n",
    "#   ice (glacier) heat capacity cv [J/m3/K] (uniform across column using this definition)\n",
    "# cv water = 4.188e6 , cv ice = 1.9415e+06     \n",
    "#     near -20 C\n",
    "nc_data['glc_cv_1d'] = np.copy(glc_cv * stretch)\n",
    "\n",
    "#   ice (glacier) thermal conductivity tk [W/m/K] (uniform across column using this definition)\n",
    "#     near -20 C\n",
    "nc_data['glc_tk_1d'] = np.copy(glc_tk * stretch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all the vars into an xarray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a time vector for months 1-12\n",
    "time_vect = range(months_per_yr + 1)[1:months_per_yr + 1]\n",
    "\n",
    "attrs = {'Units'}\n",
    "\n",
    "ds_slim = {}\n",
    "\n",
    "# Write out to the new file:\n",
    "ds_slim = xr.Dataset({'glc_mask': (['time','lsmlat','lsmlon'], nc_data['glc_mask']),\n",
    "                      'alb_gvd': (['time','lsmlat','lsmlon'], nc_data['alb_gvd']),\n",
    "                      'alb_svd': (['time','lsmlat','lsmlon'], nc_data['alb_svd']),\n",
    "                      'alb_gnd': (['time','lsmlat','lsmlon'], nc_data['alb_gnd']),\n",
    "                      'alb_snd': (['time','lsmlat','lsmlon'], nc_data['alb_snd']),\n",
    "                      'alb_gvf': (['time','lsmlat','lsmlon'], nc_data['alb_gvf']),\n",
    "                      'alb_svf': (['time','lsmlat','lsmlon'], nc_data['alb_svf']),\n",
    "                      'alb_gnf': (['time','lsmlat','lsmlon'], nc_data['alb_gnf']),\n",
    "                      'alb_snf': (['time','lsmlat','lsmlon'], nc_data['alb_snf']),\n",
    "                      'bucketdepth': (['time','lsmlat','lsmlon'], nc_data['bucketdepth']),\n",
    "                      'emissivity': (['time','lsmlat','lsmlon'], nc_data['emissivity']),\n",
    "                      'snowmask': (['time','lsmlat','lsmlon'], nc_data['snowmask']), \n",
    "                      'roughness': (['time','lsmlat','lsmlon'], nc_data['roughness']),\n",
    "                      'evap_res': (['time','lsmlat','lsmlon'], nc_data['evap_res']),\n",
    "                      'l2xavg_Fall_flxdst1': (['time','lsmlat','lsmlon'], nc_data['l2xavg_Fall_flxdst1']),\n",
    "                      'l2xavg_Fall_flxdst2': (['time','lsmlat','lsmlon'], nc_data['l2xavg_Fall_flxdst2']),\n",
    "                      'l2xavg_Fall_flxdst3': (['time','lsmlat','lsmlon'], nc_data['l2xavg_Fall_flxdst3']),\n",
    "                      'l2xavg_Fall_flxdst4': (['time','lsmlat','lsmlon'], nc_data['l2xavg_Fall_flxdst4']),\n",
    "                      'soil_type': (['time','lsmlat','lsmlon'], nc_data['soil_type']),\n",
    "                      'soil_tk_1d': (['time','lsmlat','lsmlon'], nc_data['soil_tk_1d']),\n",
    "                      'soil_cv_1d': (['time','lsmlat','lsmlon'], nc_data['soil_cv_1d']),\n",
    "                      'glc_tk_1d': (['time','lsmlat','lsmlon'], nc_data['glc_tk_1d']),\n",
    "                      'glc_cv_1d': (['time','lsmlat','lsmlon'], nc_data['glc_cv_1d'])},\n",
    "                      coords = {'lsmlon': (['lsmlon'], lon_ctsm),\n",
    "                                'lsmlat': (['lsmlat'], lat_ctsm), \n",
    "                                'time': (['time'], time_vect)},\n",
    "                      attrs = {'Author': username,\n",
    "                               'Date_created': tm.strftime(\"%Y-%m-%d %H:%M:%S\") + ' ' + tm.tzname[0] + ' ' + tm.tzname[1],\n",
    "                               'Resolution': 'see surfdat_file listed below',\n",
    "                               'Description': 'SLIM surdat file',\n",
    "                               'ccesm_source_run': casename,\n",
    "                               'ctsm_file': ctsm_file,\n",
    "                               'dust_file': dust_file,\n",
    "                               'surfdat_file_for_glc_mask': surfdat_file,\n",
    "                              }\n",
    "                    )\n",
    "\n",
    "# Define units of each variable, and map them onto the dataset\n",
    "# TODO ...something similar for other variable attributes, eg. valid_range, etc.\n",
    "# For guidance: https://cfconventions.org/Data/cf-conventions/cf-conventions-1.10/cf-conventions.html#attribute-appendix\n",
    "attr_map = {'glc_mask': ['unitless', 9.99e20], \n",
    "            'alb_gvd': ['unitless', 9.99e20], \n",
    "            'alb_svd': ['unitless', 9.99e20],\n",
    "            'alb_gnd': ['unitless', 9.99e20], \n",
    "            'alb_snd': ['unitless', 9.99e20], \n",
    "            'alb_gvf': ['unitless', 9.99e20],\n",
    "            'alb_svf': ['unitless', 9.99e20], \n",
    "            'alb_gnf': ['unitless', 9.99e20], \n",
    "            'alb_snf': ['unitless', 9.99e20],\n",
    "            'bucketdepth': ['kg/m2', 9.99e20], \n",
    "            'emissivity': ['unitless', 9.99e20], \n",
    "            'snowmask': ['kg/m2', 9.99e20],\n",
    "            'roughness': ['m', 9.99e20], \n",
    "            'evap_res': ['s/m', 9.99e20],\n",
    "            'l2xavg_Fall_flxdst1': ['unknown', 9.99e20], \n",
    "            'l2xavg_Fall_flxdst2': ['unknown', 9.99e20], \n",
    "            'l2xavg_Fall_flxdst3': ['unknown', 9.99e20], \n",
    "            'l2xavg_Fall_flxdst4': ['unknown', 9.99e20], \n",
    "            'soil_type': ['unknown', 9.99e20], \n",
    "            'soil_tk_1d': ['W/m/K', 9.99e20], \n",
    "            'soil_cv_1d': ['J/m3/K', 9.99e20], \n",
    "            'glc_tk_1d': ['W/m/K', 9.99e20], \n",
    "            'glc_cv_1d': ['J/m3/K', 9.99e20], \n",
    "            'lsmlat': ['degrees north', False],\n",
    "            'lsmlon': ['degrees east', False], \n",
    "            'time': ['month', False]}\n",
    "\n",
    "for var, val in attr_map.items():\n",
    "    ds_slim[var].attrs['Units'] = val[0]\n",
    "    ds_slim[var].attrs['_FillValue'] = val[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out the new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_slim.to_netcdf(path = 'surdat_' + tm.strftime(\"%Y%m%d\") + '.nc', format = 'NETCDF3_64BIT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ctsm_pylib]",
   "language": "python",
   "name": "conda-env-ctsm_pylib-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
